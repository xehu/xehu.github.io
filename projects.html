<!DOCTYPE html>
<html class="no-js" lang="en">
<head>

    <!— basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <title>Publications & Projects</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="css/base.css">
    <link rel="stylesheet" href="css/vendor.css">
    <link rel="stylesheet" href="css/main.css">

    <!-- script
    ================================================== -->
    <script src="js/modernizr.js"></script>

    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="icon" href="favicon.ico" type="image/x-icon">

</head>

<body id="top">

    <!-- preloader
    ================================================== -->
    <div id="preloader">
        <div id="loader" class="dots-fade">
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>


    <!-- header
    ================================================== -->
    <header class="s-header header">

        <div class="header__logo">
            <a class="logo" href="/index.html">
                <img src="/images/logo.png" alt="Homepage" width="100%" height="auto">
            </a>
        </div>

    <!-- navigation bar-->     
    <a class="header__toggle-menu" href="#0" title="Menu"><span>Menu</span></a>
    <nav class="header__nav-wrap">
    <h2 class="header__nav-heading h6">Navigate to</h2>
    <ul class="header__nav">
        <li class="current"><a href="/index.html" title="">Home</a></li>
       <li><a href="/projects.html" title="">Publications & Projects</a></li>
       <li><a href="/cv.html" title="">CV</a></li>
       <li><a href="/page-blog.html" title="">Blog</a></li>
    </ul>
     <a href="#0" title="Close Menu" class="header__overlay-close close-mobile-menu">Close</a>
    </nav> <!-- end header__nav-wrap -->

    </header>


    <!-- s-content
    ================================================== -->
    <section class="s-content s-content--top-padding s-content--narrow">

        <div class="row narrow">
            <div class="col-full s-content__header">
                <h1 class="display-1 display-1--with-line-sep">Publications & Projects.</h1>
                <p class="lead">
                This page features selected research and work in progress.
            </div>
        </div>

        <!--

        <div class="row narrow">
            <div class="col-full s-content__main">

                <center><h1 class="display-1--with-line-sep">Ongoing Projects</h1></center>

            </div>
        </div>

        -->


        <div class="row">
            <div class="col-full s-content__main">

                <br><br>
                <center><h1 class="display-1--with-line-sep">Research</h1></center>


                <!-- Begin publications section -->
                <div class="Rtable Rtable--2cols Rtable--collapse">


                <div class="Rtable-cell" style="width: 165px;">
                <img  width="150" height="150" alt="A 3D plot representing a vector space of tasks" src="images/thumbs/projects/Task Space.png">
                </div>
                <div class="Rtable-cell">
                        <h2>The Task Space: An Integrative Framework for Team Research</h2>
                        <em>Accepted at Management Science (October 2025)</em><br>
                        <b>Xinlan Emily Hu</b>, Mark E. Whiting, Linnea Gandhi, Duncan J. Watts, Abdullah Almaatouq
                        <br><br>
                        <button type="button" class="collapsible">Read More</button>
                        <div class="content" style="display: none;">
                            <p>Research on teams spans many diverse contexts, but integrating knowledge from heterogeneous sources is challenging because studies typically examine different tasks that cannot be directly compared. Most investigations involve teams working on just one or a handful of tasks, and researchers lack principled ways to quantify how similar or different these tasks are from one another. We address this challenge by introducing the “Task Space,” a multidimensional framework that represents tasks along 24 theoretically-motivated dimensions. To demonstrate its utility, we apply the Task Space to a fundamental question in teams research: when do interacting groups outperform individuals? Using the Task Space to systematically sample 20 diverse tasks, we conducted an integrative experiment with 1,231 participants working at three complexity levels, either individually or in groups of three or six (180 experimental conditions). We find striking heterogeneity in group advantage, with groups performing anywhere from three times worse to 60% better than the best individual working alone, depending on the task context. Task Space dimensions significantly outperform traditional typologies in predicting group advantage on unseen tasks. Additionally, our models reveal theoretically meaningful interactions between task features; for example, group advantage on creative tasks depends on whether the answers are objectively verifiable. The Task Space ultimately enables researchers to move beyond isolated findings to identify boundary conditions and build cumulative knowledge about team performance.</p>

                            <p><a href="https://taskmap.seas.upenn.edu/" target="_blank">Visit Project Website</a></p>
                            
                            <p><a href="https://osf.io/preprints/psyarxiv/543sz" target="_blank">Download Preprint (Please note that this link may reflect a previous version; email me for the latest manuscript!)</a></p>

                        </div>
                 </div> 

                <div class="Rtable-cell" style="width: 165px;">
                <img  width="150" height="150" alt="YouTube Filter Bubble" src="images/thumbs/projects/YouTube.png">
                </div>
                <div class="Rtable-cell">
                        <h2>Short-term exposure to filter-bubble recommendation systems has limited polarization effects: Naturalistic experiments on YouTube</h2>
                        <em>Proceedings of the National Academy of Sciences (2025)</em><br>
                        Naijia Liu, <b>Xinlan Emily Hu</b>, Yasemin Savas, Matthew A. Baum, Adam J. Berinsky, Allison J. B. Chaney, Christopher Lucas, Rei Mariman, Justin de Benedictis-Kessner, Andrew M. Guess, Dean Knox, and Brandon M. Stewart
                        <br><br>
                        <button type="button" class="collapsible">Read More</button>
                        <div class="content" style="display: none;">
                            <p>An enormous body of literature argues that recommendation algorithms drive political polarization by creating “filter bubbles” and “rabbit holes.” Using four experiments with nearly 9,000 participants, we show that manipulating algorithmic recommendations to create these conditions has limited effects on opinions. Our experiments employ a custom-built video platform with a naturalistic, YouTube-like interface presenting real YouTube videos and recommendations. We experimentally manipulate YouTube’s actual recommendation algorithm to simulate filter bubbles and rabbit holes by presenting ideologically balanced and slanted choices. Our design allows us to intervene in a feedback loop that has confounded the study of algorithmic polarization—the complex interplay between supply of recommendations and user demand for content—to examine downstream effects on policy attitudes. We use over 130,000 experimentally manipulated recommendations and 31,000 platform interactions to estimate how recommendation algorithms alter users’ media consumption decisions and, indirectly, their political attitudes. Our results cast doubt on widely circulating theories of algorithmic polarization by showing that even heavy-handed (although short-term) perturbations of real-world recommendations have limited causal effects on policy attitudes. Given our inability to detect consistent evidence for algorithmic effects, we argue the burden of proof for claims about algorithm-induced polarization has shifted. Our methodology, which captures and modifies the output of real-world recommendation algorithms, offers a path forward for future investigations of black-box artificial intelligence systems. Our findings reveal practical limits to effect sizes that are feasibly detectable in academic experiments.</p>
                            
                            <p><a href="https://www.pnas.org/doi/10.1073/pnas.2318127122" target="_blank">Download Full Paper</a></p>

                            <p><a href="https://css.seas.upenn.edu/new-study-challenges-youtubes-rabbit-hole-effect-on-political-polarization/" target="_blank">Read Press Release (February 2025)</a></p>

                        </div>
                 </div>    


                <div class="Rtable-cell" style="width: 165px;">
                <img  width="150" height="150" alt="Logo for the Team Communication Toolkit" src="images/thumbs/projects/TCT.png">
                </div>
                <div class="Rtable-cell">
                        <h2>team_comm_tools:  A Python Toolkit for Exploring the Communication Space</h2>
                        <em>Manuscript Under Review</em><br>
                        <b>Xinlan Emily Hu</b>
                        <br><br>
                        <button type="button" class="collapsible">Read More</button>
                        <div class="content" style="display: none;">
                            <p>How a team talks is a rich source of information about its collaboration process, revealing how its members resolve conflicts, coordinate activities, and monitor goal progress. However, the full potential of communication data often goes unrealized: analyzing communication requires a series of contingent decisions about which constructs to study and how to measure them. Consequently, researchers interested in team communication face a high barrier to entry, challenges with reproducibility, and constraints on their ability to capture phenomena that emerge from interactions between multiple conversational dimensions. To address these challenges, we present the Team Communication Toolkit (team_comm_tools on the Python Package Index), an open-source Python package that extracts more than 160 communication features from any corpus of text-based conversations. Our collection of measures allows the analyst to understand the effect of conversational features in the context of a broader family of features that may equivalently explain the phenomenon. Rather than forcing researchers to commit to a single measurement of complex phenomena such as constructiveness or assertiveness, the toolkit’s flexible output format makes it possible to systematically compare measures, isolate confounders, and identify higher-order interactions. We call our general analytical approach the “Communication Space,” reflecting the fact that researchers can now more easily explore the realm of possible ways to analyze conversational data. Finally, we demonstrate the utility of our tool and framework through a case study exploring conflict escalation in both real-world and synthetic data.</p>
                            
                            <p><a href="https://osf.io/preprints/psyarxiv/pz98q" target="_blank">Download Preprint</a></p>

                            <p><a href="https://css.seas.upenn.edu/the-team-communication-toolkit-emily-hus-award-winning-project/" target="_blank">Read Press Release (August 2024)</a></p>

                            <p><a href="https://penntoday.upenn.edu/news/wharton-hu-css-lab-mechanics-collaboration/" target="_blank">Read Penn Today Feature (August 2024)</a></p>

                        </div>
                 </div>   

                <div class="Rtable-cell" style="width: 165px;">
                <img  width="150" height="150" alt="Conflict" src="images/thumbs/projects/Conflict.png">
                </div>
                <div class="Rtable-cell">
                        <h2>What you say or how you say it? Predicting Conflict Outcomes in Real and LLM-Generated Conversations</h2>
                        <em>NeurIPS 2024 Behavioral ML Workshop</em><br>
                        Priya Ronald D'Costa, Evan Rowbotham, and <b>Xinlan Emily Hu</b>
                        <br><br>
                        <button type="button" class="collapsible">Read More</button>
                        <div class="content" style="display: none;">
                            <p>When conflicts escalate, is it due to what is said or how it is said? In the conflict literature, two theoretical approaches take opposing views: one focuses on the content of the disagreement, while the other focuses on how it is expressed. This paper aims to integrate these two perspectives through a computational analysis of 191 communication features -- 128 related to expression and 63 to content. We analyze 1,200 GPT-4 simulated conversations and 12,630 real-world discussions from Reddit. We find that expression features more reliably predict destructive conflict outcomes across both settings, although the most important features differ. In the Reddit data, conversational dynamics such as turn-taking and conversational equality are highly predictive, but they are not predictive in simulated conversations. These results may suggest a possible limitation in simulating social interactions with language models, and we discuss the implications for our findings on building social computing systems.</p>
                            
                            <p><a href="https://arxiv.org/pdf/2409.09338" target="_blank">Download Short Paper</a></p>

                        </div>
                 </div>    
                
                <div class="Rtable-cell" style="width: 165px;">
                        <img  width="150" height="150" alt="Distance Matters Image" src="images/thumbs/projects/Distance-Matters.png">
                </div>
                <div class="Rtable-cell">
                        <h2>A "Distance Matters" Paradox: Facilitating Intra-Team Collaboration Can Harm Inter-Team Collaboration</h2>
                        <em>CSCW 2022</em><br>
                        <b>Xinlan Emily Hu</b>, Rebecca Hinds, Melissa A. Valentine, Michael S. Bernstein
                        <br><br>
                        <button type="button" class="collapsible">Read More</button>
                        <div class="content" style="display: none;">
                            <p>By identifying the socio-technical conditions required for teams to work effectively remotely, the Distance Matters framework has been influential in CSCW since its introduction in 2000. Advances in collaboration technology and practices have since brought teams increasingly closer to achieving these conditions. This paper presents a ten-month ethnography in a remote organization, where we observed that despite exhibiting excellent remote collaboration, teams paradoxically struggled to collaborate across team boundaries. We extend the Distance Matters framework to account for inter-team collaboration, arguing that challenges analogous to those in the original intra-team framework --- common ground, collaboration readiness, collaboration technology readiness, and coupling of work --- persist but are actualized differently at the inter-team scale. Finally, we identify a fundamental tension between the intra- and inter-team layers: the collaboration technology and practices that help individual teams thrive (e.g., adopting customized collaboration software) can also prompt collaboration challenges in the inter-team layer, and conversely the technology and practices that facilitate inter-team collaboration (e.g., strong centralized IT organizations) can harm practices at the intra-team layer. The addition of the inter-team layer to the Distance Matters framework opens new opportunities for CSCW, where balancing the tension between team and organizational collaboration needs will be a critical technological, operational, and organizational challenge for remote work in the coming decades.</p>
                            
                            <p><a href="./projects/distance-matters-paradox.pdf" target="_blank">Download Full Paper</a></p>

                        </div>
                 </div>


                <div class="Rtable-cell" style="width: 165px;">
                    <img  width="150" height="150" alt="Juries Image" src="images/thumbs/projects/juries.png">
                </div>
                <div class="Rtable-cell">
                        <h2>Can Online Juries Make Consistent, Repeatable Decisions?</h2>
                         <em>CHI 2021</em><br>
                         <b>Xinlan Emily Hu</b>, Mark E. Whiting, Michael S. Bernstein
                         <br><br>

                        <button type="button" class="collapsible">Read More</button>
                        <div class="content" style="display: none;">
                          <p>A jury of one’s peers is a prominent way to adjudicate disputes and is increasingly used in participatory governance online. The fairness of this approach rests on the assumption that juries are consistent: that the same jury would hand down similar judgments to similar cases. However, prior literature suggests that social influence would instead cause early interactions to cascade into different judgments for similar cases. In this paper, we report an online experiment that changes participants’ pseudonyms as they appear to collaborators, temporarily masking a jury’s awareness that they have deliberated together before. This technique allows us to measure consistency by reconvening the same jury on similar cases. Counter to expectation, juries are equally consistent as individuals, a result that is “good for democracy.” But this consistency arises in part due to group polarization, as consensus develops by hardening initial majority opinions. Furthermore, we find that aggregating groups’ perspectives without deliberation erodes consistency.</p>

                            <p><a href="https://hci.stanford.edu/publications/paper.php?id=402" target="_blank">Download Full Paper</a></p>
                            <p><a href="https://osf.io/tv7j6/" target="_blank">View Data/Materials Released via Open Science Framework</a></p>  
                        </div>
                </div>
                
                <div class="Rtable-cell" style="width: 165px;">
                        <img  width="150" height="150" alt="Disarming Loaded Words Image" src="images/thumbs/projects/DLW.png">
                </div>
                <div class="Rtable-cell">
                         <h2>Disarming Loaded Words: Addressing Gender Bias in Political Reporting</h2>
                            <em>C+J 2021</em><br>
                            Irena Fischer-Hwang, Dylan Grosz, <b>Xinlan Emily Hu</b>, Anjini Karthik, Vivian Yang (equal contribution)
                            <br><br>

                            <button type="button" class="collapsible">Read More</button>
                            <div class="content" style="display: none;">
                                <p>
                                Gender bias has been a pervasive issue in U.S. political coverage since women gained the franchise in 1920. Women are routinely targeted for their appearance, likeability, and familial qualities—characteristics for which men are rarely scrutinized. As a way to help journalists and editors identify and correct gender-biased language in political reporting, we introduce “Disarming Loaded Words” (DLW) as a new solution to this issue. DLW is a computational tool, backed by both machine learning and human expert curation, that tags potentially biased words in a document and provides feedback and context for why a word may be problematic. DLW provides a minimally disruptive way to nudge journalists toward understanding unconscious biases. It was successfully prototyped as a standalone Google Docs add-on, and the code for its model, interface, and API are now open-source and available for use on other platforms.</p>
                                
                                <p><a href="https://cpb-us-w2.wpmucdn.com/express.northeastern.edu/dist/d/53/files/2019/11/CJ_2020_paper_68.pdf" target="_blank">Download Full Paper</a></em></p>
                            </div>
                </div>
            </div>
      
               
                <!-- COMMENT SEPARATING PUBLICATIONS FROM PROJECTS -->
                <!-- Note Jul 2024: commenting out college projects -->

               <!--  <br><br>
                <center><h1 class="display-1--with-line-sep">Projects</h1></center>

                <div class="Rtable Rtable--2cols Rtable--collapse">

                 <div class="Rtable-cell" style="width: 165px;">
                        <img  width="150" height="150" alt="Smile and Nod Image" src="images/thumbs/projects/Smile-And-Nod.png">
                </div>
                <div class="Rtable-cell">
                            <h2>Smile and Nod: Few-Shot Detection of Social Influence-Based Disingenuity in Discussions</h2>
                            <b>Xinlan Emily Hu</b>, Michael J. Cooper, Makena Low (equal contribution)
                            <br><br>
                            <button type="button" class="collapsible">Read More</button>
                            <div class="content" style="display: none;">
                                <p>
                                Deception detection is a difficult task, both for humans and machine learning models. In this paper, we contribute the novel NLU task of detecting social influence-based <em>disingenuity</em> in group discussions. We present a novel dataset for this task, on which we test three state-of-the-art machine learning models with two optimization schemes. We additionally benchmark our performance against humans. The task we propose is challenging; although several of our models outperform statistical and human baselines, overall performance leaves room to be desired. This task and dataset, therefore, remain an open challenge for NLU research.</p>
                                
                                <p><em>
                                This was our final project for the course CS224U: Natural Language Understanding at Stanford University.</em></p> 

                                <p><a href="./projects/smile-and-nod.pdf" target="_blank">Download Full Paper</a></em></p>
                            </div>
                </div>
                <div class="Rtable-cell" style="width: 165px;">
                            <img  width="150" height="150" alt="ModBotHero6 Image" src="images/thumbs/projects/ModBotHero6.png">
                </div>
                <div class="Rtable-cell">
                            <h2>ModBotHero6: A Human-Centered Content Moderation Slackbot</h2>
                            <b>Xinlan Emily Hu</b>, Matthew Trost, Jonah Wu (equal contribution)
                            <br><br>
                            <button type="button" class="collapsible">Read More</button>
                            <div class="content" style="display: none;">
                                <p>
                                As a platform, Slack has almost no built-in content moderation. On the vanilla Slack platform, it's possible to curse out your teammates, post hateful content, and disseminate misinformation with impunity.</p>
                                <p>
                                Thus, enter ModBotHero6. We combine a little bit of Reddit moderation bot with a little bit of Baymax—the friendly robot from Big Hero 6 that always asks its users whether they're 'satisfied with their care.' Like Baymax, our goal here is to put humans first. Our philosophy was to use the AI-powered moderation bot to remove or reduce immediate potential threats (i.e., hate speech, sexually explicit material, etc.) while also clearly signalling our understanding that AI isn't perfect. Some uses of profanity are positive, or appropriate given context—nuances that AI cannot yet detect. As such, the content we moderate goes to a human moderator for final approval. Each moderation report contains the API scores, in addition to a link to the message for context and bot-generated suggested actions.</p>
                                <p>
                                We also realize that AI isn't perfect at catching every instance of abuse. Some types of abuse are nuanced, and hate speech is almost always culturally-specific. As a result, this bot also includes a flow for users to report abusive content, which, in turn, also gets directed to the moderator channel.</p>
                                
                                <p><em>
                                This project was selected as one of the top two projects in the course CS 152 (Trust and Safety Engineering) at Stanford. We were featured on the Stanford Internet Observatory's Live Webinar in March 2020! Check it out
                                <a href="https://www.youtube.com/watch?v=HMtdHAd0_WY" target="_blank">here</a>.</em></p> 
                            </div>
                    </div>  
                    <div class="Rtable-cell" style="width: 165px;">
                            <img  width="150" height="150" alt="Zeal Image" src="images/thumbs/projects/Zeal.png">
                    </div>
                    <div class="Rtable-cell">
                            <h2>Zeal: Motivating Online Petitioning by Browsing the News</h2>
                            <b>Xinlan Emily Hu</b>, Angela Luo, Logan Pearce (equal contribution)
                            <br><br>
                            <button type="button" class="collapsible">Read More</button>
                            <div class="content" style="display: none;">
                                <p>Online petitions are a powerful but at times deeply flawed digital space. Despite the increasing importance of online petitions in galvanizing support for key issues, over 99% of petitions fail to achieve their goal, and the top 5% of users on Change.org (a popular online petitioning website) write 50% of the signatures. As a result, engagement in online petitions is incredibly uneven, and there are far more passive observers than active petitioners.</p>

                                <p>We introduce Zeal, a mobile application that encourages the otherwise passive consumer of content to take social action in the online petition space. Zeal combines news, a commonly consumed type of mobile content, with petitions related to the news articles. Drawing from the Fogg Behavior Model, we hypothesized that users who were invested in the news (motivated), faced with a simple ask (signing a petition), and given a prompt (Zeal’s news/petitions interface) would increase their petition-signing behavior. We then conducted a 10-day field study, through which we validated Zeal as a proof-of-concept for encouraging social action, but found that news alone is insufficient to fully motivate petition-signing behavior.</p>

                                <p><em>This project won three prizes — Best Field Study, Best Design, and Best Overall — in CS 377U: Understanding Users, evaluated by industry experts from Google, Yahoo, and Facebook.</em></p>

                                <p><a href="./projects/zeal-final-report.pdf" target="_blank">Download Project Report</a></em></p>
                            </div>
                    </div>
                    <div class="Rtable-cell" style="width: 165px;">
                            <img  width="150" height="150" alt="Vitae Image" src="images/thumbs/projects/Vitae.png">
                    </div>
                    <div class="Rtable-cell">
                            <h2>Vitae: Digital Hiring Halls for Online Crowd Workers</h2>
                            Ali Alkhatib, Gobi Dasu, Allison Chi, Jenny Han, <b>Xinlan Emily Hu</b>, Michael S. Bernstein
                            <br><br>
                            <button type="button" class="collapsible">Read More</button>
                            <div class="content" style="display: none;">
                                <p>
                                Workers on Amazon Mechanical Turk often find it impossible to prove their qualifications and find decent-paying work. Vitae is a resume management system for online gig workers, enabling them to achieve higher wages and more meaningful work.
                                </p>
                                <p><em>This project was conducted through the CURIS (Undergraduate CS Research) Program.</em></p>
                                <a href="./projects/curis-poster.pdf" target="_blank">Download Poster</a>
                            </div>
                    </div>
                </div>
 -->
                
                <!-- BEGIN SECTION OF OTHER WRITINGS --->

                <br><br>
                <center><h1 class="display-1--with-line-sep">Writing Archive</h1></center>

                <h2>Endangered Languages: Rescuing the World’s Invisible Libraries</h2>
                <button type="button" class="collapsible">Read More</button>
                <div class="content" style="display: none;">
                    <p>
                    Thousands of the world's languages disappear each year. Some sounds are never heard again. What does it mean when we lose a language—and is there a chance to save them?
                    </p>
                    <p><em>This project won the Boothe Prize for Excellence in Writing.</em></p>
                    <a href="./projects/endangered_languages.pdf" target="_blank">Download Paper</a>
                </div>

                <h2>Writings from My Time at the Oxford Internet Institute: The Political Dynamics of the Internet</h2>
                <button type="button" class="collapsible">Read More</button>
                <div class="content" style="display: none;">
                    <p>
                        I had the immense fortune of studying abroad at Oxford from April - June 2019, where I worked on five essays under an advisor at the Oxford Internet Institute.
                        <br><br>
                        <a href="posts/oxford-working-papers.html">Download Writings</a>
                    </p>
                </div>

                <hr>

            </div> <!-- s-content__main -->
        </div> <!-- end row -->

    </section> <!-- end s-content -->


    <div class="col-six">
        <div class="s-footer__copyright">
            <p style="color: lightsteelblue; font-size: 10px;"><!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | Website made with <i class="fa fa-heart" aria-hidden="true"></i> by Xinlan Emily Hu with an initial template by <a href="https://colorlib.com" target="_blank" style="color:lightsteelblue;">Colorlib</a>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            </p>
        </div>
        </div>            
    </div>
    </div> <!-- end s-footer__bottom -->

        <div class="go-top">
            <a class="smoothscroll" title="Back to Top" href="#top"></a>
        </div>

    </footer> <!-- end s-footer -->


    <!-- Java Script
    ================================================== -->
    <script src="js/jquery-3.2.1.min.js"></script>
    <script src="js/plugins.js"></script>
    <script src="js/main.js"></script>
    <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
          this.innerHTML = "Read More";
        } else {
          content.style.display = "block";
          this.innerHTML = "Read Less";
        }
      });
    }
    </script>

</body>

</html>